{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkML Logistic Regression Model\n",
    "**Goal:** Build a logistic regression model using the taxicab data set to predict payment type.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# create a spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"spark_job\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the input data file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load Data\n",
    "input_path='<path-to>/green_tripdata_2024-01.parquet'\n",
    "raw_df = spark \\\n",
    "        .read \\\n",
    "        .parquet(input_path)\n",
    "\n",
    "raw_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean and filter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for payment types 1 and 2 only\n",
    "input_df = raw_df.filter('...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Functions to vectorize input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline, Estimator\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder\n",
    "from pyspark.sql import DataFrame as SparkDataFrame\n",
    "\n",
    "# Prepare data for machine learning\n",
    "# from dataframe categorical and numeric columns create label and features\n",
    "\n",
    "def vectorizeCategories(labelCol: str, categoricalColumns:list[str]) -> list[Estimator]:\n",
    "  stages = [] # stages in Pipeline\n",
    "  for categoricalCol in categoricalColumns:\n",
    "    # Category Indexing with StringIndexer\n",
    "    stringIndexer:Estimator = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n",
    "    encoder:Estimator = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "    # Add categorical stagess\n",
    "    stages += [stringIndexer, encoder]\n",
    "  #add label category\n",
    "  label_stringIdx = StringIndexer(inputCol=labelCol, outputCol=\"label\")\n",
    "  stages += [label_stringIdx] \n",
    "  return stages\n",
    "\n",
    "def createVectorizePipeline(labelCol: str, categoricalCols:list[str], numericCols:list[str]) -> Pipeline:\n",
    "  categoricalStages = vectorizeCategories(labelCol, categoricalCols)\n",
    "  assemblerInputs = [c + \"classVec\" for c in categoricalCols] + numericCols\n",
    "  assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "  allStages:list[Estimator | VectorAssembler] = categoricalStages + [assembler]\n",
    "  partialPipeline = Pipeline().setStages(allStages) # type: ignore\n",
    "  return partialPipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select category to predict and input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label column is the feature to predict\n",
    "label_col = '<feature to predict>'\n",
    "categorical_feature_cols = [<categorical columns>]\n",
    "numeric_feature_cols=[<numeric columns>]\n",
    "vectorizePipeline = createVectorizePipeline(label_col, categorical_feature_cols, numeric_feature_cols)\n",
    "\n",
    "# create vector dataframe\n",
    "vectorizedModel = vectorizePipeline.fit(input_df)\n",
    "vectorized_df = vectorizedModel.transform(input_df)\n",
    "selectedcols = [\"label\", \"features\"] + [label_col] + categorical_feature_cols + numeric_feature_cols\n",
    "mldata_df = vectorized_df.select(selectedcols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# split the data\n",
    "train_df, test_df = mldata_df.randomSplit([0.8, 0.2])\n",
    "# select the features and label\n",
    "train_df = train_df.select(\"features\", \"label\")\n",
    "test_df = test_df.select(\"features\", \"label\")\n",
    "\n",
    "\n",
    "lrModel = lr.fit(train_df)\n",
    "prediction_df = lrModel.transform(test_df)\n",
    "prediction_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label\")\n",
    "evaluator.evaluate(prediction_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
