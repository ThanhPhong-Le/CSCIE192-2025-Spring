{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkML Logistic Regression Model\n",
    "**Goal:** Build a logistic regression model using the taxicab data set to predict payment type.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/site-packages (3.5.5)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/site-packages (from pyspark) (0.10.9.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/27 04:28:34 WARN Utils: Your hostname, codespaces-35d966 resolves to a loopback address: 127.0.0.1; using 10.0.3.203 instead (on interface eth0)\n",
      "25/03/27 04:28:34 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/27 04:28:35 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# create a spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"spark_job\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the input data file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "|VendorID|lpep_pickup_datetime|lpep_dropoff_datetime|store_and_fwd_flag|RatecodeID|PULocationID|DOLocationID|passenger_count|trip_distance|fare_amount|extra|mta_tax|tip_amount|tolls_amount|ehail_fee|improvement_surcharge|total_amount|payment_type|trip_type|congestion_surcharge|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "|       2| 2024-01-01 00:46:55|  2024-01-01 00:58:25|                 N|         1|         236|         239|              1|         1.98|       12.8|  1.0|    0.5|      3.61|         0.0|     NULL|                  1.0|       21.66|           1|        1|                2.75|\n",
      "|       2| 2024-01-01 00:31:42|  2024-01-01 00:52:34|                 N|         1|          65|         170|              5|         6.54|       30.3|  1.0|    0.5|      7.11|         0.0|     NULL|                  1.0|       42.66|           1|        1|                2.75|\n",
      "|       2| 2024-01-01 00:30:21|  2024-01-01 00:49:23|                 N|         1|          74|         262|              1|         3.08|       19.8|  1.0|    0.5|       3.0|         0.0|     NULL|                  1.0|       28.05|           1|        1|                2.75|\n",
      "|       1| 2024-01-01 00:30:20|  2024-01-01 00:42:12|                 N|         1|          74|         116|              1|          2.4|       14.2|  1.0|    1.5|       0.0|         0.0|     NULL|                  1.0|        16.7|           2|        1|                 0.0|\n",
      "|       2| 2024-01-01 00:32:38|  2024-01-01 00:43:37|                 N|         1|          74|         243|              1|         5.14|       22.6|  1.0|    0.5|      6.28|         0.0|     NULL|                  1.0|       31.38|           1|        1|                 0.0|\n",
      "|       1| 2024-01-01 00:43:41|  2024-01-01 01:00:23|                 N|         1|          33|         209|              1|          2.0|       17.0| 3.75|    1.5|       2.0|         0.0|     NULL|                  1.0|       24.25|           1|        1|                2.75|\n",
      "|       1| 2024-01-01 00:31:56|  2024-01-01 00:48:09|                 N|         1|          74|         238|              2|          3.2|       18.4| 3.75|    1.5|       4.7|         0.0|     NULL|                  1.0|       28.35|           1|        1|                2.75|\n",
      "|       2| 2024-01-01 00:46:12|  2024-01-01 00:57:39|                 N|         1|         166|         239|              2|         2.01|       13.5|  1.0|    0.5|      5.62|         0.0|     NULL|                  1.0|       24.37|           1|        1|                2.75|\n",
      "|       2| 2024-01-01 00:38:07|  2024-01-01 00:39:23|                 N|         1|         226|         226|              1|         0.31|        3.7|  1.0|    0.5|       0.0|         0.0|     NULL|                  1.0|         6.2|           2|        1|                 0.0|\n",
      "|       2| 2024-01-01 00:44:24|  2024-01-01 00:57:47|                 N|         1|           7|         129|              1|         2.32|       14.9|  1.0|    0.5|      3.48|         0.0|     NULL|                  1.0|       20.88|           1|        1|                 0.0|\n",
      "|       2| 2024-01-01 00:18:37|  2024-01-01 00:35:16|                 N|         1|          42|          75|              1|         2.69|       17.7|  1.0|    0.5|      4.04|         0.0|     NULL|                  1.0|       24.24|           1|        1|                 0.0|\n",
      "|       2| 2024-01-01 00:47:36|  2024-01-01 01:07:34|                 N|         1|          41|         141|              1|         3.73|       21.2|  1.0|    0.5|      5.29|         0.0|     NULL|                  1.0|       31.74|           1|        1|                2.75|\n",
      "|       2| 2024-01-01 00:03:57|  2024-01-01 00:18:06|                 N|         1|         130|         196|              1|         5.06|       23.3|  1.0|    0.5|      7.74|         0.0|     NULL|                  1.0|       33.54|           1|        1|                 0.0|\n",
      "|       2| 2024-01-01 00:39:58|  2024-01-01 00:58:09|                 N|         1|          74|          69|              1|         2.93|       19.1|  1.0|    0.5|      4.32|         0.0|     NULL|                  1.0|       25.92|           1|        1|                 0.0|\n",
      "|       2| 2024-01-01 00:18:52|  2024-01-01 00:25:10|                 N|         1|          41|          74|              1|         1.26|        8.6|  1.0|    0.5|       2.0|         0.0|     NULL|                  1.0|        13.1|           1|        1|                 0.0|\n",
      "|       2| 2024-01-01 00:10:33|  2024-01-01 00:22:36|                 N|         1|          55|         210|              1|         4.09|       19.1|  1.0|    0.5|       0.0|         0.0|     NULL|                  1.0|        21.6|           2|        1|                 0.0|\n",
      "|       2| 2024-01-01 00:12:54|  2024-01-01 00:20:56|                 N|         1|          41|          42|              1|         1.66|       10.7|  1.0|    0.5|       0.0|         0.0|     NULL|                  1.0|        13.2|           1|        1|                 0.0|\n",
      "|       2| 2024-01-01 00:26:23|  2024-01-01 00:34:53|                 N|         1|          42|         151|              1|         1.55|       10.7|  1.0|    0.5|      1.32|         0.0|     NULL|                  1.0|       14.52|           1|        1|                 0.0|\n",
      "|       2| 2024-01-01 00:41:05|  2024-01-01 00:50:47|                 N|         1|          41|          42|              1|         1.78|       12.1|  1.0|    0.5|       0.0|         0.0|     NULL|                  1.0|        14.6|           2|        1|                 0.0|\n",
      "|       2| 2024-01-01 00:14:03|  2024-01-01 00:19:06|                 N|         1|         255|         255|              1|         0.26|        6.5|  1.0|    0.5|       0.0|         0.0|     NULL|                  1.0|         9.0|           2|        1|                 0.0|\n",
      "+--------+--------------------+---------------------+------------------+----------+------------+------------+---------------+-------------+-----------+-----+-------+----------+------------+---------+---------------------+------------+------------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Load Data\n",
    "input_path='../data/green_tripdata_2024-01.parquet'\n",
    "raw_df = spark \\\n",
    "        .read \\\n",
    "        .parquet(input_path)\n",
    "\n",
    "raw_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean and filter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for payment types 1 and 2 only\n",
    "input_df = raw_df.filter('...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Functions to vectorize input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline, Estimator\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder\n",
    "from pyspark.sql import DataFrame as SparkDataFrame\n",
    "\n",
    "# Prepare data for machine learning\n",
    "# from dataframe categorical and numeric columns create label and features\n",
    "\n",
    "def vectorizeCategories(labelCol: str, categoricalColumns:list[str]) -> list[Estimator]:\n",
    "  stages = [] # stages in Pipeline\n",
    "  for categoricalCol in categoricalColumns:\n",
    "    # Category Indexing with StringIndexer\n",
    "    stringIndexer:Estimator = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol + \"Index\")\n",
    "    encoder:Estimator = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "    # Add categorical stagess\n",
    "    stages += [stringIndexer, encoder]\n",
    "  #add label category\n",
    "  label_stringIdx = StringIndexer(inputCol=labelCol, outputCol=\"label\")\n",
    "  stages += [label_stringIdx] \n",
    "  return stages\n",
    "\n",
    "def createVectorizePipeline(labelCol: str, categoricalCols:list[str], numericCols:list[str]) -> Pipeline:\n",
    "  categoricalStages = vectorizeCategories(labelCol, categoricalCols)\n",
    "  assemblerInputs = [c + \"classVec\" for c in categoricalCols] + numericCols\n",
    "  assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "  allStages:list[Estimator | VectorAssembler] = categoricalStages + [assembler]\n",
    "  partialPipeline = Pipeline().setStages(allStages) # type: ignore\n",
    "  return partialPipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select category to predict and input features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label column is the feature to predict\n",
    "label_col = '<feature to predict>'\n",
    "categorical_feature_cols = [<categorical columns>]\n",
    "numeric_feature_cols=[<numeric columns>]\n",
    "vectorizePipeline = createVectorizePipeline(label_col, categorical_feature_cols, numeric_feature_cols)\n",
    "\n",
    "# create vector dataframe\n",
    "vectorizedModel = vectorizePipeline.fit(input_df)\n",
    "vectorized_df = vectorizedModel.transform(input_df)\n",
    "selectedcols = [\"label\", \"features\"] + [label_col] + categorical_feature_cols + numeric_feature_cols\n",
    "mldata_df = vectorized_df.select(selectedcols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"features\", labelCol=\"label\")\n",
    "\n",
    "# split the data\n",
    "train_df, test_df = mldata_df.randomSplit([0.8, 0.2])\n",
    "# select the features and label\n",
    "train_df = train_df.select(\"features\", \"label\")\n",
    "test_df = test_df.select(\"features\", \"label\")\n",
    "\n",
    "\n",
    "lrModel = lr.fit(train_df)\n",
    "prediction_df = lrModel.transform(test_df)\n",
    "prediction_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"label\")\n",
    "evaluator.evaluate(prediction_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
